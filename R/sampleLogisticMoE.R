#' Draw a sample from a normal mixture of linear experts model.
#'
#' @param wk_star The parameters of the gating network. `wk_star` is a matrix of
#'   size \emph{(q + 1, K - 1)}, with \emph{K - 1}, the number of regressors
#'   (experts) and \emph{q} the order of the logistic regression
#' @param eta_star Array of size \emph{(K, R-1, d)} representing the regression
#'   coefficients of the experts network.
#' @param x A vector og length \emph{n} representing the inputs (predictors).
#'
#' @return A list with the output variable `y` and statistics.
#' \itemize{
#'   \item `y` Vector of length \emph{n} giving the output variable.
#'   \item `zi` A vector of size \emph{n} giving the hidden label of the
#'     expert component generating the i-th observation. Its elements are
#'     \eqn{zi[i] = k}, if the i-th observation has been generated by the
#'     k-th expert.
#'   \item `z` A matrix of size \emph{(n, K)} giving the values of the binary
#'     latent component indicators \eqn{Z_{ik}}{Zik} such that
#'     \eqn{Z_{ik} = 1}{Zik = 1} iff \eqn{Z_{i} = k}{Zi = k}.
#'  \item `stats` A list whose elements are:
#'    \itemize{
#'      \item `Ey_k` Matrix of size \emph{(n, K)} giving the conditional
#'        expectation of Yi the output variable given the value of the
#'        hidden label of the expert component generating the ith observation
#'        \emph{zi = k}, and the value of predictor \emph{X = xi}.
#'      \item `Ey` Vector of length \emph{n} giving the conditional expectation
#'        of Yi given the value of predictor \emph{X = xi}.
#'      \item `Vary_k` Vector of length \emph{k} representing the conditional
#'        variance of Yi given \emph{zi = k}, and \emph{X = xi}.
#'      \item `Vary` Vector of length \emph{n} giving the conditional expectation
#'        of Yi given \emph{X = xi}.
#'    }
#' }
#' @export
#'
#' @examples
#' n <- 500 # Size of the sample
#' wk_star <- matrix(c(0, 8), ncol = 1) # Parameters of the gating network
#' eta_star <- matrix(c(0, -2.5, 0, 2.5), ncol = 2) # Regression coefficients of the experts
#' sigmak <- c(1, 1) # Standard deviations of the experts
#' x <- seq.int(from = -1, to = 1, length.out = n) # Inputs (predictors)
#'
#' # Generate sample of size n
#' sample <- sampleUnivNMoE(wk_star = wk_star, eta_star = eta_star, sigmak = sigmak, x = x)
#'
#' # Plot points and estimated means
#' plot(x, sample$y, pch = 4)
#' lines(x, sample$stats$Ey_k[, 1], col = "blue", lty = "dotted", lwd = 1.5)
#' lines(x, sample$stats$Ey_k[, 2], col = "blue", lty = "dotted", lwd = 1.5)
#' lines(x, sample$stats$Ey, col = "red", lwd = 1.5)

sampleLogisticMoE <- function(wk_star, eta_star, x) {
  ## Test code
  n <- length(x)
  # p <- nrow(eta_star) - 1 # p<- d, beta <- (a,b)
  # q <- nrow(wk_star) - 1 # q <- d, alpha <- (beta0,beta1)
  # K = ncol(eta_star) # K <- k*
  # wk <<- matrix(0, q + 1, K - 1)
  # eta <<- array(0, dim = c(p + 1, K, R-1))
  p <- dim(eta_star)[1] - 1 # p<- d, beta <- (a,b)
  q <- dim(wk_star)[1] - 1 # q <- d, alpha <- (beta0,beta1)
  K <- dim(eta_star)[2] # K <- k*

  # Build the regression design matrices

  XBeta <- designmatrix(x, p, q)$XBeta # For the polynomial regression
  XAlpha <- designmatrix(x, p, q)$Xw # For the logistic regression

  y <- rep.int(x = 0, times = n)
  z <- zeros(n, K)
  zi <- rep.int(x = 0, times = n)

  # Calculate the mixing proportions piik:
  piik_wk <- multinomialLogit(wk_star, XAlpha, zeros(n, K), ones(n, 1))$piik

  for (i in 1:n) {
    zik_wk <- stats::rmultinom(n = 1, size = 1, piik_wk[i,])

    piik_eta_i <- multinomialLogit(as.matrix(eta_star[zik_wk == 1,,]),
                                   t(as.matrix(XAlpha[i,])), zeros(1, K), ones(1, 1))$piik

    zik_eta <- stats::rmultinom(n = 1, size = 1, piik_eta_i)

    z[i, ] <- t(zik_wk)
    zi[i] <- which.max(zik_wk)
    y[i] <- which.max(zik_eta)
  }
  return(list(y = y, zi = zi, z = z))
}

## Original sampleUnivNMoE
# sampleUnivNMoE <- function(wk_star, eta_star, sigmak, x) {
#
#   n <- length(x)
#
#   p <- nrow(eta_star) - 1
#   q <- nrow(wk_star) - 1
#   K = ncol(eta_star)
#
#   # Build the regression design matrices
#
#   XBeta <- designmatrix(x, p, q)$XBeta # For the polynomial regression
#   XAlpha <- designmatrix(x, p, q)$Xw # For the logistic regression
#
#   y <- rep.int(x = 0, times = n)
#   z <- zeros(n, K)
#   zi <- rep.int(x = 0, times = n)
#
#   # Calculate the mixing proportions piik:
#   piik <- multinomialLogit(wk_star, XAlpha, zeros(n, K), ones(n, 1))$piik
#
#   for (i in 1:n) {
#     zik <- stats::rmultinom(n = 1, size = 1, piik[i,])
#
#     mu <- as.numeric(XBeta[i,] %*% eta_star[, zik == 1])
#     sigma <- sigmak[zik == 1]
#
#     y[i] <- stats::rnorm(n = 1, mean = mu, sd = sigma)
#     z[i, ] <- t(zik)
#     zi[i] <- which.max(zik)
#
#   }
#
#   # Statistics (means, variances)
#
#   # E[yi|xi,zi=k]
#   Ey_k <- XBeta %*% eta_star
#
#   # E[yi|xi]
#   Ey <- rowSums(piik * Ey_k)
#
#   # Var[yi|xi,zi=k]
#   Vary_k <- sigmak ^ 2
#
#   # Var[yi|xi]
#   Vary <- rowSums(piik * (Ey_k ^ 2 + ones(n, 1) %*% Vary_k)) - Ey ^ 2
#
#   stats <- list()
#   stats$Ey_k <- Ey_k
#   stats$Ey <- Ey
#   stats$Vary_k <- Vary_k
#   stats$Vary <- Vary
#   # TestTin
#   stats$piik <- piik
#   stats$gate <- colSums(piik)
#   return(list(y = y, zi = zi, z = z, stats = stats))
# }
